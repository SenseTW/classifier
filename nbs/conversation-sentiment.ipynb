{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis on [z17176 dataset](https://github.com/z17176/Chinese_conversation_sentiment).\n",
    "\n",
    "This dataset was used in the following research.  They have built a 3m corpus for the research but only released the 30k dataset.\n",
    "\n",
    "* [1]L. Zhang and C. Chen, “Sentiment Classification with Convolutional Neural Networks: An Experimental Study on a Large-Scale Chinese Conversation Corpus,” in 2016 12th International Conference on Computational Intelligence and Security (CIS), 2016, pp. 165–169. http://ieeexplore.ieee.org/abstract/document/7820437/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/conversation_sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/pm5/.local/share/virtualenvs/ggv-example-1H9VW0Dl/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import os, math, re, pickle\n",
    "#import jieba\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, Dense, Flatten, Conv1D, MaxPooling1D, BatchNormalization, Dropout\n",
    "\n",
    "#jieba.set_dictionary(\"data/dict.txt.big\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train = None\n",
    "_valid = None\n",
    "\n",
    "def load_train_valid():\n",
    "    global _train, _valid\n",
    "    if _train is None:\n",
    "        _train = pd.read_csv(os.path.join(path, \"sentiment_XS_30k.txt\"))\n",
    "    if _valid is None:\n",
    "        _valid = pd.read_csv(os.path.join(path, \"sentiment_XS_test.txt\"))\n",
    "    return _train, _valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load word embedding dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path = os.path.join(path, \"dictionary.pkl\")\n",
    "\n",
    "def create_dictionary(*data):\n",
    "    phrases = {}\n",
    "    for d in data:\n",
    "        for sentence in d:\n",
    "            for ph in sentence.split(\" \"):\n",
    "                phrases[ph] = True\n",
    "    with open(os.path.join(path, \"dictionary.txt\"), \"w\") as fh:\n",
    "        fh.writelines([ ph + \"\\n\" for ph in phrases.keys() ])\n",
    "    !cd $path; mkdir -p models; ln ../fasttext/wiki.zh.bin models/wiki.zh.bin\n",
    "    !cd $path; ../../../bin/fasttext print-word-vectors models/wiki.zh.bin < dictionary.txt > dictionary.vec\n",
    "    dictionary = pd.read_csv(os.path.join(path, \"dictionary.vec\"), \n",
    "                             delim_whitespace=True, engine=\"python\", header=None, index_col=0)\n",
    "    with open(dictionary_path, \"wb\") as fh:\n",
    "        pickle.dump([{ ph: i for i, ph in enumerate(dictionary.index) }, dictionary], fh)\n",
    "\n",
    "def load_dictionary():\n",
    "    with open(dictionary_path, \"rb\") as fh:\n",
    "        [ dict_index, dictionary ] = pickle.load(fh)\n",
    "        return dict_index, dictionary\n",
    "    \n",
    "if not os.path.exists(dictionary_path):\n",
    "    train, valid = load_train_valid()\n",
    "    create_dictionary(train.text, valid.text)\n",
    "\n",
    "dict_index, dictionary = load_dictionary()\n",
    "phrases_n = len(dictionary)\n",
    "latent_n = len(dictionary.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode lables and embed phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrase-length (min, max, mean, std) = (1, 23, 4.7941782325330093, 2.0175720386692686)\n",
    "input_length = 8\n",
    "\n",
    "data_path = os.path.join(path, \"data.pkl\")\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    def get_label(df):\n",
    "        labels = df[\"labels\"].values\n",
    "        labels[labels == \"positive\"] = 1\n",
    "        labels[labels == \"negative\"] = 0\n",
    "        return labels\n",
    "\n",
    "    def get_text(df):\n",
    "        texts = np.zeros((len(df), input_length))\n",
    "        for i, text in enumerate(df.text.values):\n",
    "            for j, ph in enumerate(text.split(\" \")[:input_length]):\n",
    "                if ph in dict_index:\n",
    "                    texts[i, j] = dict_index[ph]\n",
    "        return texts\n",
    "    \n",
    "    train, valid = load_train_valid()\n",
    "    train_x, train_y = get_text(train), get_label(train)\n",
    "    valid_x, valid_y = get_text(valid), get_label(valid)\n",
    "    \n",
    "    with open(data_path, \"wb\") as fh:\n",
    "        pickle.dump([(train_x, train_y), (valid_x, valid_y)], fh)\n",
    "else:\n",
    "    with open(data_path, \"rb\") as fh:\n",
    "        [(train_x, train_y), (valid_x, valid_y)] = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 8, 300)        6644400     embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNorma (None, 8, 300)        1200        embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 8, 300)        0           batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_1 (Convolution1D)  (None, 8, 64)         57664       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 8, 64)         256         convolution1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 8, 64)         0           batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_1 (MaxPooling1D)    (None, 4, 64)         0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 256)           0           maxpooling1d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 100)           25700       flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 100)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             101         dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 6,729,321\n",
      "Trainable params: 84,193\n",
      "Non-trainable params: 6,645,128\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def simple_cnn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(phrases_n, latent_n, input_length=input_length, weights=[dictionary], trainable=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(64, 3, border_mode=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation=\"relu\"))\n",
    "    model.add(Dropout(0.7))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    return model\n",
    "    \n",
    "simple_cnn = simple_cnn_model()\n",
    "simple_cnn.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "simple_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29613 samples, validate on 11562 samples\n",
      "Epoch 1/1\n",
      "29613/29613 [==============================] - 12s - loss: 0.6369 - acc: 0.6870 - val_loss: 0.4649 - val_acc: 0.7903\n",
      "Train on 29613 samples, validate on 11562 samples\n",
      "Epoch 1/4\n",
      "29613/29613 [==============================] - 11s - loss: 0.4500 - acc: 0.7973 - val_loss: 0.3988 - val_acc: 0.8211\n",
      "Epoch 2/4\n",
      "29613/29613 [==============================] - 11s - loss: 0.4006 - acc: 0.8246 - val_loss: 0.3816 - val_acc: 0.8383\n",
      "Epoch 3/4\n",
      "29613/29613 [==============================] - 11s - loss: 0.3663 - acc: 0.8466 - val_loss: 0.3615 - val_acc: 0.8432\n",
      "Epoch 4/4\n",
      "29613/29613 [==============================] - 11s - loss: 0.3383 - acc: 0.8601 - val_loss: 0.3788 - val_acc: 0.8377\n",
      "Train on 29613 samples, validate on 11562 samples\n",
      "Epoch 1/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.3228 - acc: 0.8657 - val_loss: 0.3695 - val_acc: 0.8390\n",
      "Epoch 2/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.3109 - acc: 0.8726 - val_loss: 0.3575 - val_acc: 0.8512\n",
      "Epoch 3/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.3010 - acc: 0.8775 - val_loss: 0.3491 - val_acc: 0.8547\n",
      "Epoch 4/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2906 - acc: 0.8838 - val_loss: 0.3973 - val_acc: 0.8382\n",
      "Epoch 5/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2770 - acc: 0.8885 - val_loss: 0.3656 - val_acc: 0.8486\n",
      "Epoch 6/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2697 - acc: 0.8922 - val_loss: 0.3547 - val_acc: 0.8558\n",
      "Epoch 7/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2666 - acc: 0.8920 - val_loss: 0.3772 - val_acc: 0.8495\n",
      "Epoch 8/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2627 - acc: 0.8950 - val_loss: 0.3683 - val_acc: 0.8504\n",
      "Epoch 9/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2519 - acc: 0.8982 - val_loss: 0.3515 - val_acc: 0.8611\n",
      "Epoch 10/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2431 - acc: 0.9031 - val_loss: 0.3509 - val_acc: 0.8632\n",
      "Epoch 11/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2369 - acc: 0.9053 - val_loss: 0.3684 - val_acc: 0.8583\n",
      "Epoch 12/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2343 - acc: 0.9073 - val_loss: 0.3509 - val_acc: 0.8613\n",
      "Epoch 13/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2298 - acc: 0.9076 - val_loss: 0.3530 - val_acc: 0.8612\n",
      "Epoch 14/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2220 - acc: 0.9124 - val_loss: 0.3531 - val_acc: 0.8616\n",
      "Epoch 15/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2188 - acc: 0.9149 - val_loss: 0.3579 - val_acc: 0.8609\n",
      "Epoch 16/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2143 - acc: 0.9149 - val_loss: 0.3564 - val_acc: 0.8608\n",
      "Train on 29613 samples, validate on 11562 samples\n",
      "Epoch 1/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2135 - acc: 0.9160 - val_loss: 0.3484 - val_acc: 0.8628\n",
      "Epoch 2/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2093 - acc: 0.9178 - val_loss: 0.3611 - val_acc: 0.8594\n",
      "Epoch 3/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2073 - acc: 0.9191 - val_loss: 0.3587 - val_acc: 0.8628\n",
      "Epoch 4/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1999 - acc: 0.9213 - val_loss: 0.3746 - val_acc: 0.8640\n",
      "Epoch 5/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1990 - acc: 0.9229 - val_loss: 0.3595 - val_acc: 0.8627\n",
      "Epoch 6/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1979 - acc: 0.9207 - val_loss: 0.3663 - val_acc: 0.8607\n",
      "Epoch 7/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1937 - acc: 0.9234 - val_loss: 0.3804 - val_acc: 0.8583\n",
      "Epoch 8/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1885 - acc: 0.9266 - val_loss: 0.3627 - val_acc: 0.8646\n",
      "Epoch 9/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1886 - acc: 0.9270 - val_loss: 0.3732 - val_acc: 0.8638\n",
      "Epoch 10/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1848 - acc: 0.9260 - val_loss: 0.3939 - val_acc: 0.8607\n",
      "Epoch 11/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1838 - acc: 0.9273 - val_loss: 0.3797 - val_acc: 0.8623\n",
      "Epoch 12/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1783 - acc: 0.9300 - val_loss: 0.3642 - val_acc: 0.8671\n",
      "Epoch 13/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1822 - acc: 0.9276 - val_loss: 0.3715 - val_acc: 0.8609\n",
      "Epoch 14/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1762 - acc: 0.9306 - val_loss: 0.3734 - val_acc: 0.8587\n",
      "Epoch 15/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1733 - acc: 0.9311 - val_loss: 0.3660 - val_acc: 0.8678\n",
      "Epoch 16/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1729 - acc: 0.9316 - val_loss: 0.3755 - val_acc: 0.8673\n",
      "Train on 29613 samples, validate on 11562 samples\n",
      "Epoch 1/2\n",
      "29613/29613 [==============================] - 11s - loss: 0.1699 - acc: 0.9331 - val_loss: 0.3995 - val_acc: 0.8582\n",
      "Epoch 2/2\n",
      "29613/29613 [==============================] - 11s - loss: 0.1687 - acc: 0.9347 - val_loss: 0.3787 - val_acc: 0.8656\n"
     ]
    }
   ],
   "source": [
    "def train_simple_cnn(lr=None, epoch=1, full=False):\n",
    "    if lr is not None:\n",
    "        simple_cnn.optimizer.lr = lr\n",
    "    if full:\n",
    "        simple_cnn.layers[0].trainable = True\n",
    "    simple_cnn.fit(train_x, train_y, nb_epoch=epoch, validation_data=(valid_x, valid_y))\n",
    "    \n",
    "train_simple_cnn(1e-4)\n",
    "train_simple_cnn(1e-1, 4)\n",
    "train_simple_cnn(1e-2, 16)\n",
    "train_simple_cnn(1e-3, 16)\n",
    "train_simple_cnn(1e-4, 2, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn.save_weights(os.path.join(path, \"models\", \"simple_cnn.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_cnn\n",
    "model.load_weights(os.path.join(path, \"models\", \"simple_cnn_random.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(valid_x)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = load_train_valid()\n",
    "\n",
    "false_positive = pred > 0.6 * ~valid_y\n",
    "false_negative = pred < 0.4 * valid_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  AP 好 任性\n",
       "1                   a 甜心 一手 货源 招 代理  这个 骗子\n",
       "2                    っ ╥ ╯ ﹏ ╰ ╥ c 被 土豪 欺负\n",
       "3                       ❀ ℋ č 点点  讨厌 不许 亲亲\n",
       "4                         Don t care  大爷 此\n",
       "5                           ee 好 霸道 欺负 老实人\n",
       "6                                     E 无语\n",
       "7                                 fct 很 讨厌\n",
       "8                                   fly 白痴\n",
       "9                                Gay 鉴定 完毕\n",
       "10                      gg 管理 随波逐流  波哥 好 \n",
       "11                      gg 无尽 空虚  又 调戏 美 女\n",
       "12                                 hi 小 傻子\n",
       "13       jtituthbx bc hjjje 呀呀  干 啥 哭 啥 事情\n",
       "14                            lol 本身 就是 抄袭\n",
       "15                                   mc 麻木\n",
       "16                             MD 你们 这群 屌丝\n",
       "17                           MM 冬夜  已 病入膏肓\n",
       "18                         mm 管理 奔驰  切 不 好\n",
       "19                            mm 管理 奔驰  装傻\n",
       "20                            MM 雪 忽悠 加 骗子\n",
       "21                        Moment 花不弃 欺负 毛线\n",
       "22                               NND 掩饰 一下\n",
       "23                     NONONO 腹黑 才 不是 邪恶 呢\n",
       "24                                 NO 不好 听\n",
       "25                                   NO 发错\n",
       "26                                 OK 啦 粪土\n",
       "27                       Q at qq  尽情 调戏 呆呆\n",
       "28                                  QQ  骗子\n",
       "29                                      sb\n",
       "                       ...                \n",
       "11532                               最 喜欢 谁\n",
       "11533                         最 喜欢 他们 哪 首歌\n",
       "11534                            最 喜欢 天线宝宝\n",
       "11535                       最 喜欢 听 说 呆 蒙撒比\n",
       "11536                           最 喜欢 听 着 哥\n",
       "11537                             最 喜欢 小 i\n",
       "11538                            最 喜欢 小金 啦\n",
       "11539                            最 喜欢 这 一段\n",
       "11540                                 最 性感\n",
       "11541                              最 最最 漂亮\n",
       "11542                      醉 货 都 真的 正规 免税店\n",
       "11543                                   尊重\n",
       "11544                                遵守 规矩\n",
       "11545                           昨天 翻译 更 完美\n",
       "11546                         昨天 另外 买 就 不错\n",
       "11547                          昨天 去 洛溪 吃大餐\n",
       "11548                  昨晚 梦 挺 精彩 一瞬间 恍惚 现实\n",
       "11549                        昨晚 怎么 乐天 搞 一起\n",
       "11550                             佐助 喜欢 男人\n",
       "11551                                左右 哈哈\n",
       "11552                               做 蛋糕 嘛\n",
       "11553                                  做得好\n",
       "11554                                做 很 好\n",
       "11555          做 卡盟 没到 星期 一只 亏 但 信 一定 能 成功\n",
       "11556                              做 开心 表情\n",
       "11557                                 做 漂亮\n",
       "11558                               做 情人 吧\n",
       "11559                      做 一个 才华横溢 机器人 吧\n",
       "11560                             坐骑 能量 恢复\n",
       "11561                                坐骑 升级\n",
       "Name: text, Length: 11562, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid[\"text\"][false_positive]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6265      Accompa ╮ ° 哎 呦 哒 晚安\n",
       "6268           bot 不灵 还 得 人 哈哈\n",
       "6270                cool 组队 插件\n",
       "6271            DL 丶 无非  果断 分解\n",
       "6273               en 只要 长得 漂亮\n",
       "6282               G 晚上   都 不错\n",
       "6291              mm 岛主 伊人  晚安\n",
       "6305               mm 素素  真 靓女\n",
       "6306        mm 优雅  叫 情人 买 就是 咯\n",
       "6313              M 女 汉子  情人 谁\n",
       "6316                O ∩ ∩ O 哈哈\n",
       "6324            SS 高级 中级 分解 嘿嘿\n",
       "6331           v 句 v 哈哈 白白净净 吧\n",
       "6332                     XX 威武\n",
       "6340                 阿姐  阿姐 威武\n",
       "6343           哎 说 为什么 都 喜欢 女人\n",
       "6350                  哎 呦 不错 哦\n",
       "6351            哎 呦 不要 这么 谦虚 啦\n",
       "6352     哎 这个 社会 发进 走 太笨 没人 喜欢\n",
       "6357                         爱\n",
       "6363                      爱 美女\n",
       "6364                    爱 你们 哟\n",
       "6373                 爱情 家庭 电视剧\n",
       "6374                   爱情 经典语录\n",
       "6376                    爱情 美 好\n",
       "6377                     爱情 什么\n",
       "6378                  爱情 什么 东东\n",
       "6385                      爱人利物\n",
       "6389                      爱 喜欢\n",
       "6390                   爱 喜欢 区别\n",
       "                 ...          \n",
       "11406                 只要 有人 喜欢\n",
       "11407                 只要 助兴 就行\n",
       "11419                    智  嘿嘿\n",
       "11422                    智慧 系统\n",
       "11425                  至尊 不会 玩\n",
       "11429                      中乐透\n",
       "11431                       中意\n",
       "11434                 终于 承认 博学\n",
       "11445              周三 精英 副本  次\n",
       "11450    主人 亲爱 主人 我们 约会 吧 ＞  ＜\n",
       "11453             主宰 丶 随  才 吃货\n",
       "11455                     助威 吧\n",
       "11459                 祝你们 幸福 哦\n",
       "11464     装备 可以 给 但 破 复活 勋章 咋整\n",
       "11469                  撞 情侣 头像\n",
       "11472          准备 约会 去 里 吧 张姐 \n",
       "11486                  自己 仔细 算\n",
       "11490               自信 太多 分 一半\n",
       "11491                自要 开心 就 好\n",
       "11492                 自知之明 很 好\n",
       "11500                   嘴 干净 点\n",
       "11528                 最 喜欢 盼 盼\n",
       "11551                    左右 哈哈\n",
       "11553                      做得好\n",
       "11554                    做 很 好\n",
       "11557                     做 漂亮\n",
       "11558                   做 情人 吧\n",
       "11559          做 一个 才华横溢 机器人 吧\n",
       "11560                 坐骑 能量 恢复\n",
       "11561                    坐骑 升级\n",
       "Name: text, Length: 1437, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid[\"text\"][false_negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
