{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis on [z17176 dataset](https://github.com/z17176/Chinese_conversation_sentiment).\n",
    "\n",
    "This dataset was used in the following research.  They have built a 3m corpus for the research but only released the 30k dataset.\n",
    "\n",
    "* [1]L. Zhang and C. Chen, “Sentiment Classification with Convolutional Neural Networks: An Experimental Study on a Large-Scale Chinese Conversation Corpus,” in 2016 12th International Conference on Computational Intelligence and Security (CIS), 2016, pp. 165–169. http://ieeexplore.ieee.org/abstract/document/7820437/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/conversation_sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import os, math, re, pickle\n",
    "#import jieba\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, Dense, Flatten, Conv1D, MaxPooling1D, BatchNormalization, Dropout\n",
    "\n",
    "#jieba.set_dictionary(\"data/dict.txt.big\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train = None\n",
    "_valid = None\n",
    "\n",
    "def load_train_valid():\n",
    "    global _train, _valid\n",
    "    if _train is None:\n",
    "        _train = pd.read_csv(os.path.join(path, \"sentiment_XS_30k.txt\"))\n",
    "    if _valid is None:\n",
    "        _valid = pd.read_csv(os.path.join(path, \"sentiment_XS_test.txt\"))\n",
    "    return _train, _valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load word embedding dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path = os.path.join(path, \"dictionary.pkl\")\n",
    "\n",
    "def create_dictionary(*data):\n",
    "    phrases = {}\n",
    "    for d in data:\n",
    "        for sentence in d:\n",
    "            for ph in sentence.split(\" \"):\n",
    "                phrases[ph] = True\n",
    "    with open(os.path.join(path, \"dictionary.txt\"), \"w\") as fh:\n",
    "        fh.writelines([ ph + \"\\n\" for ph in phrases.keys() ])\n",
    "    !cd $path; mkdir -p models; ln ../fasttext/wiki.zh.bin models/wiki.zh.bin\n",
    "    !cd $path; ../../../bin/fasttext print-word-vectors models/wiki.zh.bin < dictionary.txt > dictionary.vec\n",
    "    dictionary = pd.read_csv(os.path.join(path, \"dictionary.vec\"), \n",
    "                             delim_whitespace=True, engine=\"python\", header=None, index_col=0)\n",
    "    with open(dictionary_path, \"wb\") as fh:\n",
    "        pickle.dump([{ ph: i for i, ph in enumerate(dictionary.index) }, dictionary], fh)\n",
    "\n",
    "def load_dictionary():\n",
    "    with open(dictionary_path, \"rb\") as fh:\n",
    "        [ dict_index, dictionary ] = pickle.load(fh)\n",
    "        return dict_index, dictionary\n",
    "    \n",
    "if not os.path.exists(dictionary_path):\n",
    "    train, valid = load_train_valid()\n",
    "    create_dictionary(train.text, valid.text)\n",
    "\n",
    "dict_index, dictionary = load_dictionary()\n",
    "phrases_n = len(dictionary)\n",
    "latent_n = len(dictionary.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode lables and embed phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrase-length (min, max, mean, std) = (1, 23, 4.7941782325330093, 2.0175720386692686)\n",
    "input_length = 8\n",
    "\n",
    "data_path = os.path.join(path, \"data.pkl\")\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    def get_label(df):\n",
    "        labels = df[\"labels\"].values\n",
    "        labels[labels == \"positive\"] = 1\n",
    "        labels[labels == \"negative\"] = 0\n",
    "        return labels\n",
    "\n",
    "    def get_text(df):\n",
    "        texts = np.zeros((len(df), input_length))\n",
    "        for i, text in enumerate(df.text.values):\n",
    "            for j, ph in enumerate(text.split(\" \")[:input_length]):\n",
    "                if ph in dict_index:\n",
    "                    texts[i, j] = dict_index[ph]\n",
    "        return texts\n",
    "    \n",
    "    train, valid = load_train_valid()\n",
    "    train_x, train_y = get_text(train), get_label(train)\n",
    "    valid_x, valid_y = get_text(valid), get_label(valid)\n",
    "    \n",
    "    with open(data_path, \"wb\") as fh:\n",
    "        pickle.dump([(train_x, train_y), (valid_x, valid_y)], fh)\n",
    "else:\n",
    "    with open(data_path, \"rb\") as fh:\n",
    "        [(train_x, train_y), (valid_x, valid_y)] = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 8, 300)        6644400     embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNorma (None, 8, 300)        1200        embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 8, 300)        0           batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_1 (Convolution1D)  (None, 8, 64)         57664       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 8, 64)         256         convolution1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 8, 64)         0           batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_1 (MaxPooling1D)    (None, 4, 64)         0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 256)           0           maxpooling1d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 100)           25700       flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 100)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             101         dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 6,729,321\n",
      "Trainable params: 84,193\n",
      "Non-trainable params: 6,645,128\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def simple_cnn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(phrases_n, latent_n, input_length=input_length, weights=[dictionary], trainable=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(64, 3, border_mode=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation=\"relu\"))\n",
    "    model.add(Dropout(0.7))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    return model\n",
    "    \n",
    "simple_cnn = simple_cnn_model()\n",
    "simple_cnn.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "simple_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29613 samples, validate on 11562 samples\n",
      "Epoch 1/1\n",
      "29613/29613 [==============================] - 12s - loss: 0.6369 - acc: 0.6870 - val_loss: 0.4649 - val_acc: 0.7903\n",
      "Train on 29613 samples, validate on 11562 samples\n",
      "Epoch 1/4\n",
      "29613/29613 [==============================] - 11s - loss: 0.4500 - acc: 0.7973 - val_loss: 0.3988 - val_acc: 0.8211\n",
      "Epoch 2/4\n",
      "29613/29613 [==============================] - 11s - loss: 0.4006 - acc: 0.8246 - val_loss: 0.3816 - val_acc: 0.8383\n",
      "Epoch 3/4\n",
      "29613/29613 [==============================] - 11s - loss: 0.3663 - acc: 0.8466 - val_loss: 0.3615 - val_acc: 0.8432\n",
      "Epoch 4/4\n",
      "29613/29613 [==============================] - 11s - loss: 0.3383 - acc: 0.8601 - val_loss: 0.3788 - val_acc: 0.8377\n",
      "Train on 29613 samples, validate on 11562 samples\n",
      "Epoch 1/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.3228 - acc: 0.8657 - val_loss: 0.3695 - val_acc: 0.8390\n",
      "Epoch 2/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.3109 - acc: 0.8726 - val_loss: 0.3575 - val_acc: 0.8512\n",
      "Epoch 3/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.3010 - acc: 0.8775 - val_loss: 0.3491 - val_acc: 0.8547\n",
      "Epoch 4/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2906 - acc: 0.8838 - val_loss: 0.3973 - val_acc: 0.8382\n",
      "Epoch 5/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2770 - acc: 0.8885 - val_loss: 0.3656 - val_acc: 0.8486\n",
      "Epoch 6/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2697 - acc: 0.8922 - val_loss: 0.3547 - val_acc: 0.8558\n",
      "Epoch 7/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2666 - acc: 0.8920 - val_loss: 0.3772 - val_acc: 0.8495\n",
      "Epoch 8/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2627 - acc: 0.8950 - val_loss: 0.3683 - val_acc: 0.8504\n",
      "Epoch 9/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2519 - acc: 0.8982 - val_loss: 0.3515 - val_acc: 0.8611\n",
      "Epoch 10/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2431 - acc: 0.9031 - val_loss: 0.3509 - val_acc: 0.8632\n",
      "Epoch 11/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2369 - acc: 0.9053 - val_loss: 0.3684 - val_acc: 0.8583\n",
      "Epoch 12/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2343 - acc: 0.9073 - val_loss: 0.3509 - val_acc: 0.8613\n",
      "Epoch 13/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2298 - acc: 0.9076 - val_loss: 0.3530 - val_acc: 0.8612\n",
      "Epoch 14/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2220 - acc: 0.9124 - val_loss: 0.3531 - val_acc: 0.8616\n",
      "Epoch 15/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2188 - acc: 0.9149 - val_loss: 0.3579 - val_acc: 0.8609\n",
      "Epoch 16/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2143 - acc: 0.9149 - val_loss: 0.3564 - val_acc: 0.8608\n",
      "Train on 29613 samples, validate on 11562 samples\n",
      "Epoch 1/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2135 - acc: 0.9160 - val_loss: 0.3484 - val_acc: 0.8628\n",
      "Epoch 2/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2093 - acc: 0.9178 - val_loss: 0.3611 - val_acc: 0.8594\n",
      "Epoch 3/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.2073 - acc: 0.9191 - val_loss: 0.3587 - val_acc: 0.8628\n",
      "Epoch 4/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1999 - acc: 0.9213 - val_loss: 0.3746 - val_acc: 0.8640\n",
      "Epoch 5/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1990 - acc: 0.9229 - val_loss: 0.3595 - val_acc: 0.8627\n",
      "Epoch 6/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1979 - acc: 0.9207 - val_loss: 0.3663 - val_acc: 0.8607\n",
      "Epoch 7/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1937 - acc: 0.9234 - val_loss: 0.3804 - val_acc: 0.8583\n",
      "Epoch 8/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1885 - acc: 0.9266 - val_loss: 0.3627 - val_acc: 0.8646\n",
      "Epoch 9/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1886 - acc: 0.9270 - val_loss: 0.3732 - val_acc: 0.8638\n",
      "Epoch 10/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1848 - acc: 0.9260 - val_loss: 0.3939 - val_acc: 0.8607\n",
      "Epoch 11/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1838 - acc: 0.9273 - val_loss: 0.3797 - val_acc: 0.8623\n",
      "Epoch 12/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1783 - acc: 0.9300 - val_loss: 0.3642 - val_acc: 0.8671\n",
      "Epoch 13/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1822 - acc: 0.9276 - val_loss: 0.3715 - val_acc: 0.8609\n",
      "Epoch 14/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1762 - acc: 0.9306 - val_loss: 0.3734 - val_acc: 0.8587\n",
      "Epoch 15/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1733 - acc: 0.9311 - val_loss: 0.3660 - val_acc: 0.8678\n",
      "Epoch 16/16\n",
      "29613/29613 [==============================] - 11s - loss: 0.1729 - acc: 0.9316 - val_loss: 0.3755 - val_acc: 0.8673\n",
      "Train on 29613 samples, validate on 11562 samples\n",
      "Epoch 1/2\n",
      "29613/29613 [==============================] - 11s - loss: 0.1699 - acc: 0.9331 - val_loss: 0.3995 - val_acc: 0.8582\n",
      "Epoch 2/2\n",
      "29613/29613 [==============================] - 11s - loss: 0.1687 - acc: 0.9347 - val_loss: 0.3787 - val_acc: 0.8656\n"
     ]
    }
   ],
   "source": [
    "def train_simple_cnn(lr=None, epoch=1, full=False):\n",
    "    if lr is not None:\n",
    "        simple_cnn.optimizer.lr = lr\n",
    "    if full:\n",
    "        simple_cnn.layers[0].trainable = True\n",
    "    simple_cnn.fit(train_x, train_y, nb_epoch=epoch, validation_data=(valid_x, valid_y))\n",
    "    \n",
    "train_simple_cnn(1e-4)\n",
    "train_simple_cnn(1e-1, 4)\n",
    "train_simple_cnn(1e-2, 16)\n",
    "train_simple_cnn(1e-3, 16)\n",
    "train_simple_cnn(1e-4, 2, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn.save_weights(os.path.join(path, \"models\", \"simple_cnn.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_cnn\n",
    "model.load_weights(os.path.join(path, \"models\", \"simple_cnn.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model.predict(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.54696983,  0.54946762,  0.53444225, ...,  0.55052435,\n",
       "         0.54280049,  0.54720104], dtype=float32),\n",
       " array([0, 0, 0, ..., 1, 1, 1], dtype=object))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[:, 0], valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
