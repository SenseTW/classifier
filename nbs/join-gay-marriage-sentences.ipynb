{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/join\"\n",
    "topic = \"立法方式保障\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "import jieba\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Flatten, Dropout, BatchNormalization, Conv1D, ZeroPadding1D, MaxPooling1D\n",
    "import os, math, re, pickle\n",
    "\n",
    "jieba.set_dictionary(os.path.join(\"data\", \"dict.txt.big\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = None\n",
    "\n",
    "def load_data():\n",
    "    global _data\n",
    "    if _data is None:\n",
    "        _data = pd.read_csv(os.path.join(path, topic + \"-sentences.csv\"))\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build dictionary for word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path = os.path.join(path, \"dictionary.pkl\")\n",
    "\n",
    "clean_phrase_re = re.compile(r\"[ ]+\")\n",
    "\n",
    "def cleaned_phrase(phrases):\n",
    "    for ph in phrases:\n",
    "        ph = clean_phrase_re.sub(\"\", ph)\n",
    "        if ph != \"\":\n",
    "            yield ph\n",
    "\n",
    "def create_dictionary(*data):\n",
    "    phrases = {}\n",
    "    for d in data:\n",
    "        for sentence in d:\n",
    "            for ph in cleaned_phrase(jieba.cut(str(sentence))):\n",
    "                phrases[ph] = True\n",
    "    with open(os.path.join(path, \"dictionary.txt\"), \"w\") as fh:\n",
    "        fh.writelines([ ph + \"\\n\" for ph in phrases.keys() ])\n",
    "    !cd $path; mkdir -p models; ln ../fasttext/wiki.zh.bin models/wiki.zh.bin\n",
    "    !cd $path; ~/bin/fasttext print-word-vectors models/wiki.zh.bin < dictionary.txt > dictionary.vec\n",
    "    dictionary = pd.read_csv(os.path.join(path, \"dictionary.vec\"), \n",
    "                             delim_whitespace=True, engine=\"python\", header=None, index_col=0)\n",
    "    with open(dictionary_path, \"wb\") as fh:\n",
    "        pickle.dump([{ ph: i for i, ph in enumerate(dictionary.index) }, dictionary], fh)\n",
    "\n",
    "def load_dictionary():\n",
    "    with open(dictionary_path, \"rb\") as fh:\n",
    "        [ dict_index, dictionary ] = pickle.load(fh)\n",
    "        return dict_index, dictionary\n",
    "    \n",
    "if not os.path.exists(dictionary_path):\n",
    "    data = load_data()\n",
    "    create_dictionary(data.sentence)\n",
    "\n",
    "dict_index, dictionary = load_dictionary()\n",
    "phrases_n = len(dictionary)\n",
    "latent_n = len(dictionary.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine phrase length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = load_data()\n",
    "#stat = np.frompyfunc(lambda s: len(jieba.lcut(str(s))), 1, 1)(data.sentence.values)\n",
    "#(stat.min(), stat.max(), stat.mean(), stat.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode lables and embed phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrase-length (min, max, mean, std) = (1, 1096, 17.385437090122373, 20.549680891647231)\n",
    "input_length = 20\n",
    "\n",
    "data_path = os.path.join(path, \"data.pkl\")\n",
    "\n",
    "orid_index = { \"O\": 0, \"R\": 1, \"I\": 2, \"D\": 3 }\n",
    "\n",
    "def get_label(df):\n",
    "    labels = np.zeros((len(df), 4))\n",
    "    for i, l in enumerate(df[\"orid\"]):\n",
    "        j = orid_index[l]\n",
    "        labels[i, j] = 1\n",
    "    return labels\n",
    "\n",
    "def get_text(df):\n",
    "    texts = np.zeros((len(df), input_length))\n",
    "    for i, text in enumerate(df[\"sentence\"].values):\n",
    "        for j, ph in enumerate(cleaned_phrase(jieba.lcut(str(text)))):\n",
    "            if j >= input_length:\n",
    "                break\n",
    "            if ph in dict_index:\n",
    "                texts[i, j] = dict_index[ph]\n",
    "    return texts\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    data = load_data()\n",
    "    data = data[data[\"orid\"].notnull()]\n",
    "    mask = np.random.random(len(data)) > 0.1\n",
    "    train, valid = data[mask], data[~mask]\n",
    "    train_x, train_y = get_text(train), get_label(train)\n",
    "    valid_x, valid_y = get_text(valid), get_label(valid)\n",
    "    \n",
    "    with open(data_path, \"wb\") as fh:\n",
    "        pickle.dump([(train_x, train_y), (valid_x, valid_y)], fh)\n",
    "else:\n",
    "    with open(data_path, \"rb\") as fh:\n",
    "        [(train_x, train_y), (valid_x, valid_y)] = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_11 (Embedding)         (None, 20, 300)       9825000     embedding_input_11[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_21 (BatchNorm (None, 20, 300)       1200        embedding_11[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)             (None, 20, 300)       0           batchnormalization_21[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_11 (Convolution1D) (None, 20, 64)        57664       dropout_31[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_22 (BatchNorm (None, 20, 64)        256         convolution1d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)             (None, 20, 64)        0           batchnormalization_22[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_11 (MaxPooling1D)   (None, 10, 64)        0           dropout_32[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)             (None, 640)           0           maxpooling1d_11[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_21 (Dense)                 (None, 100)           64100       flatten_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)             (None, 100)           0           dense_21[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_22 (Dense)                 (None, 4)             404         dropout_33[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 9,948,624\n",
      "Trainable params: 122,896\n",
      "Non-trainable params: 9,825,728\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def simple_cnn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(phrases_n, latent_n, input_length=input_length, weights=[dictionary], trainable=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv1D(64, 3, border_mode=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation=\"relu\"))\n",
    "    model.add(Dropout(0.7))\n",
    "    model.add(Dense(4, activation=\"sigmoid\"))\n",
    "    return model\n",
    "    \n",
    "simple_cnn = simple_cnn_model()\n",
    "simple_cnn.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "simple_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 175 samples, validate on 23 samples\n",
      "Epoch 1/1\n",
      "175/175 [==============================] - 0s - loss: 2.1160 - acc: 0.2857 - val_loss: 1.3771 - val_acc: 0.4783\n",
      "Train on 175 samples, validate on 23 samples\n",
      "Epoch 1/4\n",
      "175/175 [==============================] - 0s - loss: 1.6770 - acc: 0.3086 - val_loss: 1.3731 - val_acc: 0.5652\n",
      "Epoch 2/4\n",
      "175/175 [==============================] - 0s - loss: 1.8868 - acc: 0.3257 - val_loss: 1.3697 - val_acc: 0.5652\n",
      "Epoch 3/4\n",
      "175/175 [==============================] - 0s - loss: 1.6635 - acc: 0.3486 - val_loss: 1.3666 - val_acc: 0.5652\n",
      "Epoch 4/4\n",
      "175/175 [==============================] - 0s - loss: 1.5803 - acc: 0.3371 - val_loss: 1.3641 - val_acc: 0.5652\n",
      "Train on 175 samples, validate on 23 samples\n",
      "Epoch 1/8\n",
      "175/175 [==============================] - 0s - loss: 1.4688 - acc: 0.3771 - val_loss: 1.3623 - val_acc: 0.5652\n",
      "Epoch 2/8\n",
      "175/175 [==============================] - 0s - loss: 1.3744 - acc: 0.4000 - val_loss: 1.3609 - val_acc: 0.5652\n",
      "Epoch 3/8\n",
      "175/175 [==============================] - 0s - loss: 1.2600 - acc: 0.4514 - val_loss: 1.3591 - val_acc: 0.5652\n",
      "Epoch 4/8\n",
      "175/175 [==============================] - 0s - loss: 1.4883 - acc: 0.3886 - val_loss: 1.3568 - val_acc: 0.5652\n",
      "Epoch 5/8\n",
      "175/175 [==============================] - 0s - loss: 1.3300 - acc: 0.4229 - val_loss: 1.3545 - val_acc: 0.5652\n",
      "Epoch 6/8\n",
      "175/175 [==============================] - 0s - loss: 1.2389 - acc: 0.4343 - val_loss: 1.3530 - val_acc: 0.5652\n",
      "Epoch 7/8\n",
      "175/175 [==============================] - 0s - loss: 1.3957 - acc: 0.4686 - val_loss: 1.3511 - val_acc: 0.6087\n",
      "Epoch 8/8\n",
      "175/175 [==============================] - 0s - loss: 1.3279 - acc: 0.4686 - val_loss: 1.3492 - val_acc: 0.6087\n",
      "Train on 175 samples, validate on 23 samples\n",
      "Epoch 1/8\n",
      "175/175 [==============================] - 0s - loss: 1.2471 - acc: 0.4571 - val_loss: 1.3473 - val_acc: 0.6087\n",
      "Epoch 2/8\n",
      "175/175 [==============================] - 0s - loss: 1.2504 - acc: 0.4571 - val_loss: 1.3461 - val_acc: 0.6087\n",
      "Epoch 3/8\n",
      "175/175 [==============================] - 0s - loss: 1.2636 - acc: 0.4971 - val_loss: 1.3454 - val_acc: 0.6087\n",
      "Epoch 4/8\n",
      "175/175 [==============================] - 0s - loss: 1.2163 - acc: 0.5029 - val_loss: 1.3440 - val_acc: 0.6087\n",
      "Epoch 5/8\n",
      "175/175 [==============================] - 0s - loss: 1.3223 - acc: 0.4914 - val_loss: 1.3438 - val_acc: 0.6087\n",
      "Epoch 6/8\n",
      "175/175 [==============================] - 0s - loss: 1.1522 - acc: 0.5429 - val_loss: 1.3441 - val_acc: 0.6087\n",
      "Epoch 7/8\n",
      "175/175 [==============================] - 0s - loss: 1.2886 - acc: 0.5143 - val_loss: 1.3442 - val_acc: 0.6087\n",
      "Epoch 8/8\n",
      "175/175 [==============================] - 0s - loss: 1.1876 - acc: 0.4971 - val_loss: 1.3446 - val_acc: 0.6087\n",
      "Train on 175 samples, validate on 23 samples\n",
      "Epoch 1/2\n",
      "175/175 [==============================] - 0s - loss: 1.0883 - acc: 0.5257 - val_loss: 1.3448 - val_acc: 0.6087\n",
      "Epoch 2/2\n",
      "175/175 [==============================] - 0s - loss: 1.1664 - acc: 0.5429 - val_loss: 1.3455 - val_acc: 0.6087\n"
     ]
    }
   ],
   "source": [
    "def train_simple_cnn(lr=None, epoch=1, full=False):\n",
    "    if lr is not None:\n",
    "        simple_cnn.optimizer.lr = lr\n",
    "    if full:\n",
    "        simple_cnn.layers[0].trainable = True\n",
    "    simple_cnn.fit(train_x, train_y, nb_epoch=epoch, batch_size=batch_size, validation_data=(valid_x, valid_y))\n",
    "    \n",
    "train_simple_cnn(1e-4)\n",
    "train_simple_cnn(1e-1, 4)\n",
    "train_simple_cnn(1e-2, 8)\n",
    "train_simple_cnn(1e-3, 8)\n",
    "train_simple_cnn(1e-4, 2, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn.save_weights(os.path.join(path, \"models\", \"simple_cnn.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_cnn_model()\n",
    "model.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.load_weights(os.path.join(path, \"models\", \"simple_cnn.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_path = os.path.join(path, \"testing.pkl\")\n",
    "\n",
    "_testing = None\n",
    "\n",
    "def load_testing():\n",
    "    if not os.path.exists(testing_path):\n",
    "        data = load_data()\n",
    "        data = data[data[\"orid\"].isnull()]\n",
    "        _testing = get_text(data)\n",
    "        with open(testing_path, \"wb\") as fh:\n",
    "            pickle.dump(_testing, fh)\n",
    "        return _testing\n",
    "    else:\n",
    "        with open(testing_path, \"rb\") as fh:\n",
    "            return pickle.load(fh)\n",
    "\n",
    "data = load_data()\n",
    "data = data[data[\"orid\"].isnull()]\n",
    "testing = load_testing()\n",
    "pred = model.predict(testing, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.43110189,  0.5061273 ,  0.57010287,  0.49707159],\n",
       "       [ 0.46737763,  0.52265048,  0.56997561,  0.51680809],\n",
       "       [ 0.46599391,  0.48684579,  0.53707284,  0.5031637 ],\n",
       "       ..., \n",
       "       [ 0.50346696,  0.48446396,  0.53563052,  0.50433385],\n",
       "       [ 0.46854225,  0.50290263,  0.5925886 ,  0.49854097],\n",
       "       [ 0.47502753,  0.47676295,  0.53359419,  0.46701998]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3237     而婚姻的與時俱進包含: 1 禁止同<姓>結婚 理由為防止近親性交  2 過去夫權獨大 3 所...\n",
       "28082    而婚姻的與時俱進包含: 1 禁止同<姓>結婚 理由為防止近親性交  2 過去夫權獨大 3 所...\n",
       "36836    然而婚姻的與時俱進包含: 1 禁止同<姓>結婚 理由為防止近親性交  2 過去夫權獨大 3 ...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[pred[:, 0] > 0.6].sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9770               <  是怎麼製造的，同志本來就不會和異性結婚，那到底你說的單親家庭是從哪裡來的\n",
       "15429     : 台灣誤家盟 < 誤家盟 > :  ,    ,      : 法務部同性婚姻民調，請大...\n",
       "22897                                               <-你的邏輯\n",
       "24268                              正在爭啊 保障同性戀者的權益 << 目前並沒有\n",
       "25755                                 <用平等的心,把每一個人擁入憲法的懷抱>\n",
       "34251    <補充>因為我國尚未承認同性婚姻,經過國際人權專家提出建議和指正應該積極更正社會歧視問題 如...\n",
       "41252                    請尊重不同聲音言論自由 年 月 日 上午 於     < >寫道：\n",
       "42010                                                    <\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[pred[:, 1] > 0.6].sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43                          >什麼是成熟 與 不成熟的 感情觀 輕描淡寫>甚至不要教更好\n",
       "123      我解釋一下好了, 免得亂掉 > <在一般的異性戀生殖中, 若有一方因疾病或是其他因素而導致不...\n",
       "908                                     輿論害死多少家庭，中國人直是禮教吃人\n",
       "969                                    不是炮反對票，是炮那些腦子已經腐化的人\n",
       "1420                                    大人們也應該如此做啊，才是以身作則吧\n",
       "1774                              為什麼 討厭同性戀 三個字不能 大纍纍的說出來呢\n",
       "2309                                   根據法務部過去的做法，這是很有可能的事\n",
       "2362                 必竟不是自己骨肉<那異性戀領養的也是有性侵案例啊,你到底要出來反領養了沒啊\n",
       "2826                                   只能拿來騙騙3歲小孩和無知的熱血青年喔\n",
       "3237     而婚姻的與時俱進包含: 1 禁止同<姓>結婚 理由為防止近親性交  2 過去夫權獨大 3 所...\n",
       "3598                                   只能拿來騙騙3歲小孩和無知的熱血青年喔\n",
       "3647                                      上帝是愛 是不是跟你想的差更多了\n",
       "3729                                      回覆:不爽同性戀那請問您來做甚麼\n",
       "3842                                             變裝慾->哪裡不好\n",
       "4752                                   我沒擔心古人，以色列人，你又胡說八道了\n",
       "5040                                   只能拿來騙騙3歲小孩和無知的熱血青年喔\n",
       "5264                                    這一點沒錯同性戀去病化 是在 年的事\n",
       "5402                                   只能拿來騙騙3歲小孩和無知的熱血青年喔\n",
       "5697                                   跟你講不要用油性潤滑劑啊,要用水性的啊\n",
       "5825                               除了人身攻擊與抹黑別人, 你何時才要恢復理智呢\n",
       "5839                                            聖水晶，給你愛 <3\n",
       "5869                                      別理這些王雪紅了、浪費時間 <3\n",
       "6412                                     上帝的善意被你這樣誤解他會打你的呦\n",
       "6489                             >這些數據資料 同運學者絕對都有 卻 完全隻字不提\n",
       "8408                   不然就是數據即為接近>至於也採開放式性教育的英美國家 數據就更不用比了\n",
       "9042                                     是要導正什麼啊不然你一個月是打幾泡\n",
       "9051                                阿啦啦啦啦啦啦啦啦啊啦啦啦啦哈哈哈啊哈哈哈哈\n",
       "9498                                今天星期五晚上，最後的投票期限，肯定會很熱鬧\n",
       "9562                                     謝謝你之前的回應，我不會再回應你了\n",
       "10512                                     所以說，阻止同性婚姻的人是邪惡的\n",
       "                               ...                        \n",
       "33256                                  只能拿來騙騙3歲小孩和無知的熱血青年喔\n",
       "33259                                   上帝只創作男人和女人，那 是誰創造的\n",
       "33277                   信不信主的這不是我的信仰，我有我自己的信仰，我也覺得我的信仰很有靈性\n",
       "33330                                 老梗,這麼想公投自己去連署啊有人擋著你嗎\n",
       "33382                              有一堆基督徒在地獄當墊背的真不錯啊 ○ ω ○\n",
       "33421                                   他們的思念變為虛妄，無知的心就昏暗了\n",
       "33511              社會上認為婚前性關係交交朋友就要同居，男女朋友不上床無法交往<  干同性戀何事\n",
       "33642                                  只能拿來騙騙3歲小孩和無知的熱血青年喔\n",
       "34009                                  既然是喜好问题，那么一个人的喜好有错么\n",
       "34605                                 只要政府派發更多福利給單身貴族就可以了啊\n",
       "34885                                      跳針 搜尋謝金燕你就找的到了喔\n",
       "35044                                他神秘地笑： 我們的會計薪水是比別人高沒錯\n",
       "35745                                    同性戀可有婚姻，那兄妹戀或姐弟戀呢\n",
       "36263                                   你還沒回答育幼院的問題捏，神會生氣喔\n",
       "36617                                  只能拿來騙騙3歲小孩和無知的熱血青年喔\n",
       "36836    然而婚姻的與時俱進包含: 1 禁止同<姓>結婚 理由為防止近親性交  2 過去夫權獨大 3 ...\n",
       "37058                              那個什麼護家盟啊 下福盟啊 不都是基督教團體嗎\n",
       "37965                                如果你覺得自殺是人權，那祝你一路好走，不送\n",
       "39711                                 現今政府社會福利政策才是少子化的最大原因\n",
       "40124                             同性戀並沒有殺人放火或者像小三一樣插足別人的家庭\n",
       "40423                              同性婚姻違反國際人權公約所保護的男女婚姻制度喔\n",
       "40774                                  只能拿來騙騙3歲小孩和無知的熱血青年喔\n",
       "41019                                 阿這個棄養女兒的可不可以叫她把錢吐出來啊\n",
       "41252                    請尊重不同聲音言論自由 年 月 日 上午 於     < >寫道：\n",
       "41255                                            變性慾->哪裡不好\n",
       "42790                                  只能拿來騙騙3歲小孩和無知的熱血青年喔\n",
       "43294                                年 月 日 下午 於     < >寫道：\n",
       "43374                                        >結果你立刻甩了A 選擇B\n",
       "43715    迄今   的研究結果顯示未能在男性或女性受訪者的全基因組識別任何的基因位點        p...\n",
       "43980                                我們是萬物之靈，不是像動物看齊，我們不是獸\n",
       "Name: sentence, Length: 138, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[pred[:, 2] > 0.6].sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123      我解釋一下好了, 免得亂掉 > <在一般的異性戀生殖中, 若有一方因疾病或是其他因素而導致不...\n",
       "5869                                      別理這些王雪紅了、浪費時間 <3\n",
       "11053                                      下午 於     < >寫道：\n",
       "15244     也就是說你根本反收養吧不然有這種異性戀傾向 又領養到不是親生骨肉的 <領養就不會是親生的啦...\n",
       "15429     : 台灣誤家盟 < 誤家盟 > :  ,    ,      : 法務部同性婚姻民調，請大...\n",
       "19427                                    >>>正在積極面對卻一直被窮追猛打\n",
       "22089                                      下午 於     < >寫道：\n",
       "23505    祝福大家都有情人終成眷屬： 單身也幸福唷    >w<  不論結果如何，願過往的傷痛隨風而去...\n",
       "24354                                    上午  於     < > 寫道：\n",
       "31640                                              我快笑死> <\n",
       "33136                          上帝愛你們 年 月 日 下午 於     < >寫道：\n",
       "42367           >現代婚姻會有這麼多問題>不是因為異性戀 或 同性戀的問題>而是情慾流動 卻不知節制\n",
       "43294                                年 月 日 下午 於     < >寫道：\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[pred[:, 3] > 0.6].sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
