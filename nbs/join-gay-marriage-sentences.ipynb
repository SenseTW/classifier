{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/join\"\n",
    "topic = \"立法方式保障\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "import jieba\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Flatten, Dropout, BatchNormalization, Conv1D, ZeroPadding1D, MaxPooling1D\n",
    "import os, math, re, pickle\n",
    "\n",
    "jieba.set_dictionary(os.path.join(\"data\", \"dict.txt.big\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = None\n",
    "\n",
    "def load_data():\n",
    "    global _data\n",
    "    if _data is None:\n",
    "        _data = pd.read_csv(os.path.join(path, topic + \"-sentences.csv\"))\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build dictionary for word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path = os.path.join(path, \"dictionary.pkl\")\n",
    "\n",
    "clean_phrase_re = re.compile(r\"[ ]+\")\n",
    "\n",
    "def cleaned_phrase(phrases):\n",
    "    for ph in phrases:\n",
    "        ph = clean_phrase_re.sub(\"\", ph)\n",
    "        if ph != \"\":\n",
    "            yield ph\n",
    "\n",
    "def create_dictionary(*data):\n",
    "    phrases = {}\n",
    "    for d in data:\n",
    "        for sentence in d:\n",
    "            for ph in cleaned_phrase(jieba.cut(str(sentence))):\n",
    "                phrases[ph] = True\n",
    "    with open(os.path.join(path, \"dictionary.txt\"), \"w\") as fh:\n",
    "        fh.writelines([ ph + \"\\n\" for ph in phrases.keys() ])\n",
    "    !cd $path; mkdir -p models; ln ../fasttext/wiki.zh.bin models/wiki.zh.bin\n",
    "    !cd $path; ~/bin/fasttext print-word-vectors models/wiki.zh.bin < dictionary.txt > dictionary.vec\n",
    "    dictionary = pd.read_csv(os.path.join(path, \"dictionary.vec\"), \n",
    "                             delim_whitespace=True, engine=\"python\", header=None, index_col=0)\n",
    "    with open(dictionary_path, \"wb\") as fh:\n",
    "        pickle.dump([{ ph: i for i, ph in enumerate(dictionary.index) }, dictionary], fh)\n",
    "\n",
    "def load_dictionary():\n",
    "    with open(dictionary_path, \"rb\") as fh:\n",
    "        [ dict_index, dictionary ] = pickle.load(fh)\n",
    "        return dict_index, dictionary\n",
    "    \n",
    "if not os.path.exists(dictionary_path):\n",
    "    data = load_data()\n",
    "    create_dictionary(data.sentence)\n",
    "\n",
    "dict_index, dictionary = load_dictionary()\n",
    "phrases_n = len(dictionary)\n",
    "latent_n = len(dictionary.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine phrase length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = load_data()\n",
    "#stat = np.frompyfunc(lambda s: len(jieba.lcut(str(s))), 1, 1)(data.sentence.values)\n",
    "#(stat.min(), stat.max(), stat.mean(), stat.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode lables and embed phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /Users/pm5/src/trustableai/ggv-example/nbs/data/dict.txt.big ...\n",
      "Loading model from cache /var/folders/sy/q12w5xyn4lngqxh_j63vwr4h0000gn/T/jieba.u6fcb61346ce78c4353b892e9bfaaea38.cache\n",
      "Loading model cost 1.651 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# phrase-length (min, max, mean, std) = (1, 1096, 17.385437090122373, 20.549680891647231)\n",
    "input_length = 20\n",
    "\n",
    "data_path = os.path.join(path, \"data.pkl\")\n",
    "\n",
    "orid_index = { \"O\": 0, \"R\": 1, \"I\": 2, \"D\": 3 }\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    def get_label(df):\n",
    "        labels = np.zeros((len(df), 4))\n",
    "        for i, l in enumerate(df[\"orid\"]):\n",
    "            j = orid_index[l]\n",
    "            labels[i, j] = 1\n",
    "        return labels\n",
    "\n",
    "    def get_text(df):\n",
    "        texts = np.zeros((len(df), input_length))\n",
    "        for i, text in enumerate(df[\"sentence\"].values):\n",
    "            for j, ph in enumerate(cleaned_phrase(jieba.lcut(str(text)))):\n",
    "                if j >= input_length:\n",
    "                    break\n",
    "                if ph in dict_index:\n",
    "                    texts[i, j] = dict_index[ph]\n",
    "        return texts\n",
    "    \n",
    "    data = load_data()\n",
    "    data = data[data[\"orid\"].notnull()]\n",
    "    mask = np.random.random(len(data)) > 0.1\n",
    "    train, valid = data[mask], data[~mask]\n",
    "    train_x, train_y = get_text(train), get_label(train)\n",
    "    valid_x, valid_y = get_text(valid), get_label(valid)\n",
    "    \n",
    "    with open(data_path, \"wb\") as fh:\n",
    "        pickle.dump([(train_x, train_y), (valid_x, valid_y)], fh)\n",
    "else:\n",
    "    with open(data_path, \"rb\") as fh:\n",
    "        [(train_x, train_y), (valid_x, valid_y)] = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 20, 300)       9825000     embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNorma (None, 20, 300)       1200        embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 20, 300)       0           batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_1 (Convolution1D)  (None, 20, 64)        57664       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 20, 64)        256         convolution1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 20, 64)        0           batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_1 (MaxPooling1D)    (None, 10, 64)        0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 640)           0           maxpooling1d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 100)           64100       flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 100)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 4)             404         dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 9,948,624\n",
      "Trainable params: 122,896\n",
      "Non-trainable params: 9,825,728\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def simple_cnn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(phrases_n, latent_n, input_length=input_length, weights=[dictionary], trainable=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(64, 3, border_mode=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation=\"relu\"))\n",
    "    model.add(Dropout(0.7))\n",
    "    model.add(Dense(4, activation=\"softmax\"))\n",
    "    return model\n",
    "    \n",
    "simple_cnn = simple_cnn_model()\n",
    "simple_cnn.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "simple_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "55/55 [==============================] - 0s - loss: 0.5324 - acc: 0.7273 - val_loss: 1.3791 - val_acc: 0.5000\n",
      "Train on 55 samples, validate on 6 samples\n",
      "Epoch 1/4\n",
      "55/55 [==============================] - 0s - loss: 0.4421 - acc: 0.8182 - val_loss: 1.3790 - val_acc: 0.3333\n",
      "Epoch 2/4\n",
      "55/55 [==============================] - 0s - loss: 0.5415 - acc: 0.8000 - val_loss: 1.3793 - val_acc: 0.3333\n",
      "Epoch 3/4\n",
      "55/55 [==============================] - 0s - loss: 0.3577 - acc: 0.8545 - val_loss: 1.3796 - val_acc: 0.3333\n",
      "Epoch 4/4\n",
      "55/55 [==============================] - 0s - loss: 0.7684 - acc: 0.6909 - val_loss: 1.3792 - val_acc: 0.3333\n",
      "Train on 55 samples, validate on 6 samples\n",
      "Epoch 1/16\n",
      "55/55 [==============================] - 0s - loss: 0.4029 - acc: 0.8545 - val_loss: 1.3793 - val_acc: 0.3333\n",
      "Epoch 2/16\n",
      "55/55 [==============================] - 0s - loss: 0.4068 - acc: 0.8364 - val_loss: 1.3793 - val_acc: 0.3333\n",
      "Epoch 3/16\n",
      "55/55 [==============================] - 0s - loss: 0.4823 - acc: 0.7818 - val_loss: 1.3808 - val_acc: 0.3333\n",
      "Epoch 4/16\n",
      "55/55 [==============================] - 0s - loss: 0.2780 - acc: 0.8909 - val_loss: 1.3810 - val_acc: 0.3333\n",
      "Epoch 5/16\n",
      "55/55 [==============================] - 0s - loss: 0.2861 - acc: 0.8909 - val_loss: 1.3808 - val_acc: 0.5000\n",
      "Epoch 6/16\n",
      "55/55 [==============================] - 0s - loss: 0.2665 - acc: 0.9091 - val_loss: 1.3804 - val_acc: 0.3333\n",
      "Epoch 7/16\n",
      "55/55 [==============================] - 0s - loss: 0.3606 - acc: 0.8364 - val_loss: 1.3796 - val_acc: 0.5000\n",
      "Epoch 8/16\n",
      "55/55 [==============================] - 0s - loss: 0.2827 - acc: 0.9091 - val_loss: 1.3790 - val_acc: 0.5000\n",
      "Epoch 9/16\n",
      "55/55 [==============================] - 0s - loss: 0.3203 - acc: 0.8727 - val_loss: 1.3797 - val_acc: 0.5000\n",
      "Epoch 10/16\n",
      "55/55 [==============================] - 0s - loss: 0.2843 - acc: 0.8909 - val_loss: 1.3802 - val_acc: 0.5000\n",
      "Epoch 11/16\n",
      "55/55 [==============================] - 0s - loss: 0.3273 - acc: 0.8909 - val_loss: 1.3815 - val_acc: 0.5000\n",
      "Epoch 12/16\n",
      "55/55 [==============================] - 0s - loss: 0.4333 - acc: 0.8182 - val_loss: 1.3848 - val_acc: 0.5000\n",
      "Epoch 13/16\n",
      "55/55 [==============================] - 0s - loss: 0.5175 - acc: 0.8000 - val_loss: 1.3874 - val_acc: 0.3333\n",
      "Epoch 14/16\n",
      "55/55 [==============================] - 0s - loss: 0.5784 - acc: 0.8364 - val_loss: 1.3903 - val_acc: 0.3333\n",
      "Epoch 15/16\n",
      "55/55 [==============================] - 0s - loss: 0.2793 - acc: 0.8909 - val_loss: 1.3917 - val_acc: 0.3333\n",
      "Epoch 16/16\n",
      "55/55 [==============================] - 0s - loss: 0.2407 - acc: 0.9273 - val_loss: 1.3920 - val_acc: 0.3333\n",
      "Train on 55 samples, validate on 6 samples\n",
      "Epoch 1/16\n",
      "55/55 [==============================] - 0s - loss: 0.4701 - acc: 0.8364 - val_loss: 1.3924 - val_acc: 0.3333\n",
      "Epoch 2/16\n",
      "55/55 [==============================] - 0s - loss: 0.2313 - acc: 0.8909 - val_loss: 1.3920 - val_acc: 0.1667\n",
      "Epoch 3/16\n",
      "55/55 [==============================] - 0s - loss: 0.4096 - acc: 0.8545 - val_loss: 1.3913 - val_acc: 0.1667\n",
      "Epoch 4/16\n",
      "55/55 [==============================] - 0s - loss: 0.2862 - acc: 0.8909 - val_loss: 1.3920 - val_acc: 0.1667\n",
      "Epoch 5/16\n",
      "55/55 [==============================] - 0s - loss: 0.4924 - acc: 0.8727 - val_loss: 1.3935 - val_acc: 0.1667\n",
      "Epoch 6/16\n",
      "55/55 [==============================] - 0s - loss: 0.2693 - acc: 0.8727 - val_loss: 1.3939 - val_acc: 0.1667\n",
      "Epoch 7/16\n",
      "55/55 [==============================] - 0s - loss: 0.3091 - acc: 0.8909 - val_loss: 1.3934 - val_acc: 0.1667\n",
      "Epoch 8/16\n",
      "55/55 [==============================] - 0s - loss: 0.2392 - acc: 0.9091 - val_loss: 1.3922 - val_acc: 0.1667\n",
      "Epoch 9/16\n",
      "55/55 [==============================] - 0s - loss: 0.4134 - acc: 0.7818 - val_loss: 1.3933 - val_acc: 0.1667\n",
      "Epoch 10/16\n",
      "55/55 [==============================] - 0s - loss: 0.2156 - acc: 0.9091 - val_loss: 1.3934 - val_acc: 0.1667\n",
      "Epoch 11/16\n",
      "55/55 [==============================] - 0s - loss: 0.3757 - acc: 0.8364 - val_loss: 1.3921 - val_acc: 0.1667\n",
      "Epoch 12/16\n",
      "55/55 [==============================] - 0s - loss: 0.2142 - acc: 0.9091 - val_loss: 1.3905 - val_acc: 0.1667\n",
      "Epoch 13/16\n",
      "55/55 [==============================] - 0s - loss: 0.4059 - acc: 0.8364 - val_loss: 1.3892 - val_acc: 0.1667\n",
      "Epoch 14/16\n",
      "55/55 [==============================] - 0s - loss: 0.2730 - acc: 0.8909 - val_loss: 1.3893 - val_acc: 0.1667\n",
      "Epoch 15/16\n",
      "55/55 [==============================] - 0s - loss: 0.2217 - acc: 0.9273 - val_loss: 1.3899 - val_acc: 0.1667\n",
      "Epoch 16/16\n",
      "55/55 [==============================] - 0s - loss: 0.2101 - acc: 0.9455 - val_loss: 1.3902 - val_acc: 0.1667\n",
      "Train on 55 samples, validate on 6 samples\n",
      "Epoch 1/2\n",
      "55/55 [==============================] - 0s - loss: 0.3183 - acc: 0.8727 - val_loss: 1.3913 - val_acc: 0.1667\n",
      "Epoch 2/2\n",
      "55/55 [==============================] - 0s - loss: 0.3577 - acc: 0.8364 - val_loss: 1.3923 - val_acc: 0.1667\n"
     ]
    }
   ],
   "source": [
    "def train_simple_cnn(lr=None, epoch=1, full=False):\n",
    "    if lr is not None:\n",
    "        simple_cnn.optimizer.lr = lr\n",
    "    if full:\n",
    "        simple_cnn.layers[0].trainable = True\n",
    "    simple_cnn.fit(train_x, train_y, nb_epoch=epoch, validation_data=(valid_x, valid_y))\n",
    "    \n",
    "train_simple_cnn(1e-4)\n",
    "train_simple_cnn(1e-1, 4)\n",
    "train_simple_cnn(1e-2, 16)\n",
    "train_simple_cnn(1e-3, 16)\n",
    "train_simple_cnn(1e-4, 2, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
