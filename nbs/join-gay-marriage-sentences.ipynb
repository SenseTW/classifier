{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/join\"\n",
    "topic = \"立法方式保障\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "import jieba\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Flatten, Dropout, BatchNormalization, Conv1D, ZeroPadding1D, MaxPooling1D\n",
    "import os, math, re, pickle\n",
    "\n",
    "jieba.set_dictionary(os.path.join(\"data\", \"dict.txt.big\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = None\n",
    "\n",
    "def load_data():\n",
    "    global _data\n",
    "    if _data is None:\n",
    "        _data = pd.read_csv(os.path.join(path, topic + \"-sentences.csv\"))\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build dictionary for word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path = os.path.join(path, \"dictionary.pkl\")\n",
    "\n",
    "clean_phrase_re = re.compile(r\"[ ]+\")\n",
    "\n",
    "def cleaned_phrase(phrases):\n",
    "    for ph in phrases:\n",
    "        ph = clean_phrase_re.sub(\"\", ph)\n",
    "        if ph != \"\":\n",
    "            yield ph\n",
    "\n",
    "def create_dictionary(*data):\n",
    "    phrases = {}\n",
    "    for d in data:\n",
    "        for sentence in d:\n",
    "            for ph in cleaned_phrase(jieba.cut(str(sentence))):\n",
    "                phrases[ph] = True\n",
    "    with open(os.path.join(path, \"dictionary.txt\"), \"w\") as fh:\n",
    "        fh.writelines([ ph + \"\\n\" for ph in phrases.keys() ])\n",
    "    !cd $path; mkdir -p models; ln ../fasttext/wiki.zh.bin models/wiki.zh.bin\n",
    "    !cd $path; ~/bin/fasttext print-word-vectors models/wiki.zh.bin < dictionary.txt > dictionary.vec\n",
    "    dictionary = pd.read_csv(os.path.join(path, \"dictionary.vec\"), \n",
    "                             delim_whitespace=True, engine=\"python\", header=None, index_col=0)\n",
    "    with open(dictionary_path, \"wb\") as fh:\n",
    "        pickle.dump([{ ph: i for i, ph in enumerate(dictionary.index) }, dictionary], fh)\n",
    "\n",
    "def load_dictionary():\n",
    "    with open(dictionary_path, \"rb\") as fh:\n",
    "        [ dict_index, dictionary ] = pickle.load(fh)\n",
    "        return dict_index, dictionary\n",
    "    \n",
    "if not os.path.exists(dictionary_path):\n",
    "    data = load_data()\n",
    "    create_dictionary(data.sentence)\n",
    "\n",
    "dict_index, dictionary = load_dictionary()\n",
    "phrases_n = len(dictionary)\n",
    "latent_n = len(dictionary.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine phrase length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = load_data()\n",
    "#stat = np.frompyfunc(lambda s: len(jieba.lcut(str(s))), 1, 1)(data.sentence.values)\n",
    "#(stat.min(), stat.max(), stat.mean(), stat.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode lables and embed phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrase-length (min, max, mean, std) = (1, 1096, 17.385437090122373, 20.549680891647231)\n",
    "input_length = 20\n",
    "\n",
    "data_path = os.path.join(path, \"data.pkl\")\n",
    "\n",
    "orid_index = { \"O\": 0, \"R\": 1, \"I\": 2, \"D\": 3 }\n",
    "\n",
    "def get_label(df):\n",
    "    labels = np.zeros((len(df), 4))\n",
    "    for i, l in enumerate(df[\"orid\"]):\n",
    "        j = orid_index[l]\n",
    "        labels[i, j] = 1\n",
    "    return labels\n",
    "\n",
    "def get_text(df):\n",
    "    texts = np.zeros((len(df), input_length))\n",
    "    for i, text in enumerate(df[\"sentence\"].values):\n",
    "        for j, ph in enumerate(cleaned_phrase(jieba.lcut(str(text)))):\n",
    "            if j >= input_length:\n",
    "                break\n",
    "            if ph in dict_index:\n",
    "                texts[i, j] = dict_index[ph]\n",
    "    return texts\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    data = load_data()\n",
    "    data = data[data[\"orid\"].notnull()]\n",
    "    mask = np.random.random(len(data)) > 0.1\n",
    "    train, valid = data[mask], data[~mask]\n",
    "    train_x, train_y = get_text(train), get_label(train)\n",
    "    valid_x, valid_y = get_text(valid), get_label(valid)\n",
    "    \n",
    "    with open(data_path, \"wb\") as fh:\n",
    "        pickle.dump([(train_x, train_y), (valid_x, valid_y)], fh)\n",
    "else:\n",
    "    with open(data_path, \"rb\") as fh:\n",
    "        [(train_x, train_y), (valid_x, valid_y)] = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_13 (Embedding)         (None, 20, 300)       9825000     embedding_input_13[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_25 (BatchNorm (None, 20, 300)       1200        embedding_13[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)             (None, 20, 300)       0           batchnormalization_25[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_13 (Convolution1D) (None, 20, 64)        57664       dropout_37[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_26 (BatchNorm (None, 20, 64)        256         convolution1d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)             (None, 20, 64)        0           batchnormalization_26[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_13 (MaxPooling1D)   (None, 10, 64)        0           dropout_38[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)             (None, 640)           0           maxpooling1d_13[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_25 (Dense)                 (None, 100)           64100       flatten_13[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)             (None, 100)           0           dense_25[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_26 (Dense)                 (None, 4)             404         dropout_39[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 9,948,624\n",
      "Trainable params: 122,896\n",
      "Non-trainable params: 9,825,728\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def simple_cnn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(phrases_n, latent_n, input_length=input_length, weights=[dictionary], trainable=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(64, 3, border_mode=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation=\"relu\"))\n",
    "    model.add(Dropout(0.7))\n",
    "    model.add(Dense(4, activation=\"softmax\"))\n",
    "    return model\n",
    "    \n",
    "simple_cnn = simple_cnn_model()\n",
    "simple_cnn.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "simple_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 181 samples, validate on 17 samples\n",
      "Epoch 1/1\n",
      "181/181 [==============================] - 1s - loss: 3.1476 - acc: 0.2210 - val_loss: 1.4112 - val_acc: 0.1176\n",
      "Train on 181 samples, validate on 17 samples\n",
      "Epoch 1/4\n",
      "181/181 [==============================] - 0s - loss: 2.7716 - acc: 0.2818 - val_loss: 1.4077 - val_acc: 0.0588\n",
      "Epoch 2/4\n",
      "181/181 [==============================] - 0s - loss: 2.7141 - acc: 0.2431 - val_loss: 1.4005 - val_acc: 0.0588\n",
      "Epoch 3/4\n",
      "181/181 [==============================] - 0s - loss: 2.4590 - acc: 0.3536 - val_loss: 1.3949 - val_acc: 0.0588\n",
      "Epoch 4/4\n",
      "181/181 [==============================] - 0s - loss: 2.1359 - acc: 0.3867 - val_loss: 1.3902 - val_acc: 0.1765\n",
      "Train on 181 samples, validate on 17 samples\n",
      "Epoch 1/12\n",
      "181/181 [==============================] - 0s - loss: 2.0767 - acc: 0.4254 - val_loss: 1.3849 - val_acc: 0.2353\n",
      "Epoch 2/12\n",
      "181/181 [==============================] - 0s - loss: 1.7785 - acc: 0.4696 - val_loss: 1.3797 - val_acc: 0.2941\n",
      "Epoch 3/12\n",
      "181/181 [==============================] - 0s - loss: 1.6846 - acc: 0.5249 - val_loss: 1.3760 - val_acc: 0.2353\n",
      "Epoch 4/12\n",
      "181/181 [==============================] - 0s - loss: 1.6011 - acc: 0.5193 - val_loss: 1.3735 - val_acc: 0.2353\n",
      "Epoch 5/12\n",
      "181/181 [==============================] - 0s - loss: 1.6770 - acc: 0.4696 - val_loss: 1.3718 - val_acc: 0.2353\n",
      "Epoch 6/12\n",
      "181/181 [==============================] - 0s - loss: 1.6731 - acc: 0.4862 - val_loss: 1.3705 - val_acc: 0.2353\n",
      "Epoch 7/12\n",
      "181/181 [==============================] - 0s - loss: 1.4480 - acc: 0.5193 - val_loss: 1.3710 - val_acc: 0.2353\n",
      "Epoch 8/12\n",
      "181/181 [==============================] - 0s - loss: 1.4913 - acc: 0.5580 - val_loss: 1.3709 - val_acc: 0.2353\n",
      "Epoch 9/12\n",
      "181/181 [==============================] - 0s - loss: 1.3361 - acc: 0.5691 - val_loss: 1.3708 - val_acc: 0.2353\n",
      "Epoch 10/12\n",
      "181/181 [==============================] - 0s - loss: 1.3305 - acc: 0.5359 - val_loss: 1.3706 - val_acc: 0.2353\n",
      "Epoch 11/12\n",
      "181/181 [==============================] - 0s - loss: 1.1695 - acc: 0.6133 - val_loss: 1.3708 - val_acc: 0.2941\n",
      "Epoch 12/12\n",
      "181/181 [==============================] - 0s - loss: 1.2292 - acc: 0.5912 - val_loss: 1.3704 - val_acc: 0.2941\n",
      "Train on 181 samples, validate on 17 samples\n",
      "Epoch 1/8\n",
      "181/181 [==============================] - 0s - loss: 1.2245 - acc: 0.6022 - val_loss: 1.3690 - val_acc: 0.3529\n",
      "Epoch 2/8\n",
      "181/181 [==============================] - 0s - loss: 1.3327 - acc: 0.5249 - val_loss: 1.3674 - val_acc: 0.3529\n",
      "Epoch 3/8\n",
      "181/181 [==============================] - 0s - loss: 1.0504 - acc: 0.5746 - val_loss: 1.3658 - val_acc: 0.3529\n",
      "Epoch 4/8\n",
      "181/181 [==============================] - 0s - loss: 1.0117 - acc: 0.6133 - val_loss: 1.3636 - val_acc: 0.3529\n",
      "Epoch 5/8\n",
      "181/181 [==============================] - 0s - loss: 0.8891 - acc: 0.6188 - val_loss: 1.3613 - val_acc: 0.3529\n",
      "Epoch 6/8\n",
      "181/181 [==============================] - 0s - loss: 0.9548 - acc: 0.5967 - val_loss: 1.3596 - val_acc: 0.3529\n",
      "Epoch 7/8\n",
      "181/181 [==============================] - 0s - loss: 1.0012 - acc: 0.6685 - val_loss: 1.3589 - val_acc: 0.3529\n",
      "Epoch 8/8\n",
      "181/181 [==============================] - 0s - loss: 0.9445 - acc: 0.6188 - val_loss: 1.3573 - val_acc: 0.3529\n",
      "Train on 181 samples, validate on 17 samples\n",
      "Epoch 1/2\n",
      "181/181 [==============================] - 0s - loss: 0.7021 - acc: 0.7072 - val_loss: 1.3555 - val_acc: 0.3529\n",
      "Epoch 2/2\n",
      "181/181 [==============================] - 0s - loss: 0.8514 - acc: 0.6961 - val_loss: 1.3542 - val_acc: 0.3529\n"
     ]
    }
   ],
   "source": [
    "def train_simple_cnn(lr=None, epoch=1, full=False):\n",
    "    if lr is not None:\n",
    "        simple_cnn.optimizer.lr = lr\n",
    "    if full:\n",
    "        simple_cnn.layers[0].trainable = True\n",
    "    simple_cnn.fit(train_x, train_y, nb_epoch=epoch, batch_size=batch_size, validation_data=(valid_x, valid_y))\n",
    "    \n",
    "train_simple_cnn(1e-4)\n",
    "train_simple_cnn(1e-1, 4)\n",
    "train_simple_cnn(1e-2, 8)\n",
    "train_simple_cnn(1e-3, 8)\n",
    "train_simple_cnn(1e-4, 2, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn.save_weights(os.path.join(path, \"models\", \"simple_cnn.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_cnn_model()\n",
    "model.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.load_weights(os.path.join(path, \"models\", \"simple_cnn.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "data = data[data[\"orid\"].isnull()]\n",
    "testing_x = get_text(data)\n",
    "pred = model.predict(testing, batchsize=batchsize)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
