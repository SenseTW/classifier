{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/join\"\n",
    "topic = \"立法方式保障\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "import jieba\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Flatten, Dropout, BatchNormalization, Conv1D, ZeroPadding1D\n",
    "import os, math, re, pickle\n",
    "\n",
    "jieba.set_dictionary(os.path.join(\"data\", \"dict.txt.big\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = None\n",
    "\n",
    "def load_data():\n",
    "    global _data\n",
    "    if _data is None:\n",
    "        _data = pd.read_csv(os.path.join(path, topic + \"-sentences.csv\"))\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build dictionary for word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: models/wiki.zh.bin: File exists\r\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = os.path.join(path, \"dictionary.pkl\")\n",
    "\n",
    "clean_phrase_re = re.compile(r\"[ ]+\")\n",
    "\n",
    "def cleaned_phrase(phrases):\n",
    "    for ph in phrases:\n",
    "        ph = clean_phrase_re.sub(\"\", ph)\n",
    "        if ph != \"\":\n",
    "            yield ph\n",
    "\n",
    "def create_dictionary(*data):\n",
    "    phrases = {}\n",
    "    for d in data:\n",
    "        for sentence in d:\n",
    "            for ph in cleaned_phrase(jieba.cut(str(sentence))):\n",
    "                phrases[ph] = True\n",
    "    with open(os.path.join(path, \"dictionary.txt\"), \"w\") as fh:\n",
    "        fh.writelines([ ph + \"\\n\" for ph in phrases.keys() ])\n",
    "    !cd $path; mkdir -p models; ln ../fasttext/wiki.zh.bin models/wiki.zh.bin\n",
    "    !cd $path; ~/bin/fasttext print-word-vectors models/wiki.zh.bin < dictionary.txt > dictionary.vec\n",
    "    dictionary = pd.read_csv(os.path.join(path, \"dictionary.vec\"), \n",
    "                             delim_whitespace=True, engine=\"python\", header=None, index_col=0)\n",
    "    with open(dictionary_path, \"wb\") as fh:\n",
    "        pickle.dump([{ ph: i for i, ph in enumerate(dictionary.index) }, dictionary], fh)\n",
    "\n",
    "def load_dictionary():\n",
    "    with open(dictionary_path, \"rb\") as fh:\n",
    "        [ dict_index, dictionary ] = pickle.load(fh)\n",
    "        return dict_index, dictionary\n",
    "    \n",
    "if not os.path.exists(dictionary_path):\n",
    "    data = load_data()\n",
    "    create_dictionary(data.sentence)\n",
    "\n",
    "dict_index, dictionary = load_dictionary()\n",
    "phrases_n = len(dictionary)\n",
    "latent_n = len(dictionary.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine phrase length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = load_data()\n",
    "#stat = np.frompyfunc(lambda s: len(jieba.lcut(str(s))), 1, 1)(data.sentence.values)\n",
    "#(stat.min(), stat.max(), stat.mean(), stat.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode lables and embed phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrase-length (min, max, mean, std) = (1, 1096, 17.385437090122373, 20.549680891647231)\n",
    "input_length = 20\n",
    "\n",
    "data_path = os.path.join(path, \"data.pkl\")\n",
    "\n",
    "orid_index = { \"O\": 0, \"R\": 1, \"I\": 2, \"D\": 3 }\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    def get_label(df):\n",
    "        labels = np.zeros((len(df), 4))\n",
    "        for i, l in enumerate(df[\"orid\"]):\n",
    "            j = orid_index[l]\n",
    "            labels[i, j] = 1\n",
    "        return labels\n",
    "\n",
    "    def get_text(df):\n",
    "        texts = np.zeros((len(df), input_length))\n",
    "        for i, text in enumerate(df[\"sentence\"].values):\n",
    "            for j, ph in enumerate(cleaned_phrase(jieba.lcut(str(text)))):\n",
    "                if j >= input_length:\n",
    "                    break\n",
    "                if ph in dict_index:\n",
    "                    texts[i, j] = dict_index[ph]\n",
    "        return texts\n",
    "    \n",
    "    data = load_data()\n",
    "    data = data[data[\"orid\"].notnull()]\n",
    "    mask = np.random.random(len(data)) > 0.1\n",
    "    train, valid = data[mask], data[~mask]\n",
    "    train_x, train_y = get_text(train), get_label(train)\n",
    "    valid_x, valid_y = get_text(valid), get_label(valid)\n",
    "    \n",
    "    with open(data_path, \"wb\") as fh:\n",
    "        pickle.dump([(train_x, train_y), (valid_x, valid_y)], fh)\n",
    "else:\n",
    "    with open(data_path, \"rb\") as fh:\n",
    "        [(train_x, train_y), (valid_x, valid_y)] = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
